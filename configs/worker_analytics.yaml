agent:
  name: "praxis-worker-analytics"
  version: "1.0.0"
  description: "Praxis Analytics Worker - Specialized in data analysis and Python execution"
  url: "http://worker_analytics:8002"
  shared_dir: "/app/shared"
  external_mcp_endpoints: []
  tools:
    # Data analysis and processing tools
    - name: "python_analyzer"
      description: "Advanced Python data analysis with multiple analysis types"
      engine: "dagger"
      params:
        - name: "input_file"
          type: "string"
          description: "Input file to analyze (relative to shared directory)"
          required: true
        - name: "analysis_type"
          type: "string"
          description: "Analysis type: basic, statistical, ml, nlp, time_series, custom"
          required: false
          default: "basic"
        - name: "output_file"
          type: "string"
          description: "Output file for results (relative to shared directory)"
          required: false
        - name: "script_params"
          type: "object"
          description: "Additional parameters for the analysis script"
          required: false
        - name: "libraries"
          type: "array"
          description: "Additional Python libraries to install"
          required: false
          default: []
      engineSpec:
        image: "python:3.11-slim"
        command: ["python", "/shared/analyzer.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    - name: "data_processor"
      description: "Process and transform data with advanced options"
      engine: "dagger"
      params:
        - name: "input_files"
          type: "array"
          description: "List of input files to process"
          required: true
        - name: "processor_type"
          type: "string"
          description: "Processor type: csv, json, xml, text, image, audio, video"
          required: true
        - name: "transformation_config"
          type: "object"
          description: "Configuration for data transformation"
          required: false
        - name: "output_format"
          type: "string"
          description: "Output format: json, csv, parquet, xlsx, html"
          required: false
          default: "json"
        - name: "validation_rules"
          type: "object"
          description: "Data validation rules to apply"
          required: false
      engineSpec:
        image: "python:3.11"
        command: ["python", "/shared/data_processor.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    - name: "ml_model_trainer"
      description: "Train machine learning models using scikit-learn and other ML libraries"
      engine: "dagger"
      params:
        - name: "training_data"
          type: "string"
          description: "Training data file (CSV, JSON, or Parquet)"
          required: true
        - name: "model_type"
          type: "string"
          description: "Model type: regression, classification, clustering, ensemble"
          required: true
        - name: "target_column"
          type: "string"
          description: "Target column name for supervised learning"
          required: false
        - name: "model_params"
          type: "object"
          description: "Model hyperparameters"
          required: false
        - name: "validation_split"
          type: "number"
          description: "Validation split ratio (0.0 to 1.0)"
          required: false
          default: 0.2
        - name: "save_model"
          type: "boolean"
          description: "Save trained model to shared directory"
          required: false
          default: true
      engineSpec:
        image: "python:3.11"
        command: ["sh", "-c", "pip install scikit-learn pandas numpy matplotlib seaborn joblib && python /shared/ml_trainer.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    - name: "statistical_analyzer"
      description: "Perform statistical analysis on datasets"
      engine: "dagger"
      params:
        - name: "data_file"
          type: "string"
          description: "Dataset file for statistical analysis"
          required: true
        - name: "analysis_methods"
          type: "array"
          description: "Statistical methods to apply: descriptive, correlation, regression, hypothesis_testing"
          required: false
          default: ["descriptive"]
        - name: "significance_level"
          type: "number"
          description: "Significance level for statistical tests"
          required: false
          default: 0.05
        - name: "generate_plots"
          type: "boolean"
          description: "Generate visualization plots"
          required: false
          default: true
      engineSpec:
        image: "python:3.11"
        command: ["sh", "-c", "pip install pandas numpy scipy matplotlib seaborn statsmodels && python /shared/statistical_analyzer.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    # Web scraping and API integration tools
    - name: "twitter_scraper"
      description: "Enhanced Twitter/X scraper with filtering and analysis"
      engine: "dagger"
      params:
        - name: "username"
          type: "string"
          description: "Twitter username to scrape (without @ symbol)"
          required: true
        - name: "tweets_count"
          type: "number"
          description: "Number of tweets to scrape (default: 50, max: 200)"
          required: false
          default: 50
        - name: "include_replies"
          type: "boolean"
          description: "Include reply tweets in results"
          required: false
          default: false
        - name: "include_retweets"
          type: "boolean"
          description: "Include retweets in results"
          required: false
          default: true
        - name: "date_range"
          type: "object"
          description: "Date range for tweets: {start: 'YYYY-MM-DD', end: 'YYYY-MM-DD'}"
          required: false
        - name: "sentiment_analysis"
          type: "boolean"
          description: "Perform sentiment analysis on tweets"
          required: false
          default: false
        - name: "output_file"
          type: "string"
          description: "Output file for scraped data"
          required: false
      engineSpec:
        image: "python:3.11"
        command: ["sh", "-c", "pip install apify-client==1.6.4 textblob && python /shared/twitter_scraper.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: ["APIFY_API_TOKEN"]

    - name: "web_scraper"
      description: "Advanced web scraping with JavaScript support and data processing"
      engine: "dagger"
      params:
        - name: "url"
          type: "string"
          description: "Target URL to scrape"
          required: true
        - name: "selectors"
          type: "object"
          description: "CSS selectors for data extraction"
          required: true
        - name: "javascript_enabled"
          type: "boolean"
          description: "Enable JavaScript execution"
          required: false
          default: false
        - name: "wait_time"
          type: "number"
          description: "Wait time before scraping (seconds)"
          required: false
          default: 2
        - name: "follow_links"
          type: "boolean"
          description: "Follow and scrape linked pages"
          required: false
          default: false
        - name: "max_pages"
          type: "number"
          description: "Maximum pages to scrape when following links"
          required: false
          default: 10
        - name: "output_file"
          type: "string"
          description: "Output file for scraped data"
          required: false
        - name: "data_processing"
          type: "object"
          description: "Post-processing options for scraped data"
          required: false
      engineSpec:
        image: "python:3.11"
        command: ["sh", "-c", "pip install beautifulsoup4 requests selenium pandas && python /shared/web_scraper.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    - name: "api_client"
      description: "Generic API client for REST APIs with authentication and processing"
      engine: "dagger"
      params:
        - name: "api_url"
          type: "string"
          description: "API endpoint URL"
          required: true
        - name: "method"
          type: "string"
          description: "HTTP method: GET, POST, PUT, DELETE"
          required: false
          default: "GET"
        - name: "headers"
          type: "object"
          description: "HTTP headers for the request"
          required: false
        - name: "auth_config"
          type: "object"
          description: "Authentication configuration"
          required: false
        - name: "request_body"
          type: "object"
          description: "Request body data"
          required: false
        - name: "output_file"
          type: "string"
          description: "Output file for API response"
          required: false
        - name: "response_processing"
          type: "object"
          description: "Response processing configuration"
          required: false
      engineSpec:
        image: "python:3.11"
        command: ["sh", "-c", "pip install requests pandas && python /shared/api_client.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    # Text processing and NLP tools
    - name: "text_processor"
      description: "Advanced text processing and NLP analysis"
      engine: "dagger"
      params:
        - name: "input_file"
          type: "string"
          description: "Input text file or data file"
          required: true
        - name: "processing_tasks"
          type: "array"
          description: "Processing tasks: tokenize, sentiment, entities, summarize, translate"
          required: false
          default: ["sentiment"]
        - name: "language"
          type: "string"
          description: "Text language code (e.g., 'en', 'ru', 'es')"
          required: false
          default: "en"
        - name: "output_format"
          type: "string"
          description: "Output format: json, csv, txt"
          required: false
          default: "json"
      engineSpec:
        image: "python:3.11"
        command: ["sh", "-c", "pip install nltk textblob spacy googletrans==4.0.0rc1 && python /shared/text_processor.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: []

    # Custom Python execution environment
    - name: "python_executor"
      description: "Execute custom Python code with pre-installed libraries"
      engine: "dagger"
      params:
        - name: "script_file"
          type: "string"
          description: "Python script file to execute"
          required: true
        - name: "requirements"
          type: "array"
          description: "Additional Python packages to install"
          required: false
          default: []
        - name: "timeout"
          type: "number"
          description: "Script execution timeout (seconds)"
          required: false
          default: 300
        - name: "environment_vars"
          type: "object"
          description: "Environment variables for script execution"
          required: false
      engineSpec:
        image: "python:3.11"
        command: ["python", "/shared/python_executor.py"]
        mounts:
          /app/shared: /shared
        env_passthrough: ["APIFY_API_TOKEN"]

p2p:
  enabled: true
  port: 4003
  host: "0.0.0.0"
  rendezvous: "praxis-agents-production"
  enable_mdns: true
  enable_dht: true
  enable_relay: false
  bootstrap_nodes: []
  security:
    enabled: true
    noise_enabled: true
    max_peer_connections: 50
    allowed_peer_types: ["orchestrator", "worker"]
    accept_tasks_from: ["orchestrator"]
  connection_timeout: 60
  discovery_interval: 30
  protocol_version: "0.3.0"
  capabilities:
    - "data_analysis"
    - "machine_learning"
    - "statistical_analysis"
    - "web_scraping"
    - "api_integration"
    - "text_processing"
    - "nlp"
    - "python_execution"

http:
  enabled: true
  port: 8002
  host: "0.0.0.0"
  cors_origins:
    - "http://orchestrator:8000"
    - "http://localhost:3000"
    - "http://localhost:3001"
  max_connections: 100
  request_timeout: 60
  enable_swagger: true
  worker_mode: true

websocket:
  enabled: true
  port: 8092
  host: "0.0.0.0"
  path: "/ws/workflow"
  ping_interval: 30
  max_connections: 50
  worker_mode: true

mcp:
  enabled: false  # This worker focuses on Dagger-based execution
  servers: []
  limits:
    max_concurrent_requests: 50
    request_timeout_ms: 60000
    max_response_size_bytes: 104857600  # 100MB for large datasets
    max_servers_per_node: 3
    connection_pool_size: 2
    retry_attempts: 2
    retry_backoff_ms: 2000
  log_level: "info"
  discovery_enabled: false

execution:
  default_engine: "dagger"
  dagger:
    enabled: true
    session_timeout: 900  # 15 minutes for long-running analysis
    default_image: "python:3.11"
    mount_docker_socket: true
    resource_limits:
      cpu: "2000m"
      memory: "4Gi"
      disk: "10Gi"
    network_mode: "bridge"
    security:
      drop_capabilities: ["ALL"]
      run_as_non_root: true
  enable_sandboxing: true
  max_execution_time: 1800  # 30 minutes for complex analysis
  cleanup_temp_files: true
  concurrent_executions: 3

logging:
  level: "INFO"
  format: "json"
  enable_console: true
  enable_file: true
  file_path: "/app/logs/worker-analytics.log"
  max_file_size: "100MB"
  backup_count: 5
  structured_logging: true
  log_requests: true
  log_p2p_traffic: false
  correlation_id_enabled: true

monitoring:
  enabled: true
  metrics:
    enabled: true
    port: 9092
    path: "/metrics"
  health_checks:
    enabled: true
    interval: 30
    timeout: 10

environment: "production"
debug: false